# The Problem with AI Safety
Killian Steunou

## Table of Contents

- [1. Introduction](#1-introduction)
	- [1.1 The promise and peril of AI](#11-the-promise-and-peril-of-ai)
	- [1.2 The current state of the debate](#12-the-current-state-of-the-debate)
- [2. Can superintelligence even exist?](#2-can-superintelligence-even-exist)
	- [2.1 Skepticism About AI Superintelligence](#21-skepticism-about-ai-superintelligence)
	- [2.2 Why superintelligence remains elusive](#22-why-superintelligence-remains-elusive)
- [3. How Imminent Is the Risk?](#3-how-imminent-is-the-risk)
	- [3.1 Why predictions of AGI are premature](#31-why-predictions-of-agi-are-premature)
	- [3.2 The problem with predictive timelines](#32-the-problem-with-predictive-timelines)
- [4. How dangerous is AI, really?](#4-how-dangerous-is-ai-really)
	- [4.1 Misuse risks](#41-misuse-risks)
		- [4.1.1 Examples of misuse](#411-examples-of-misuse)
		- [4.1.2 Are some risks exaggerated?](#412-are-some-risks-exaggerated)
	- [4.2 Conceptual issues: defining safety and alignment](#42-conceptual-issues-defining-safety-and-alignment)
	- [4.3 Why intelligence alone may not be that dangerous](#43-why-intelligence-alone-may-not-be-that-dangerous)
- [5. Neglected dimensions of AI safety](#5-neglected-dimensions-of-ai-safety)
	- [5.1 Societal and regulatory oversights](#51-societal-and-regulatory-oversights)
	- [5.2 Environmental costs](#52-environmental-costs)
- [6. AI coordination: a game-changer or a risk multiplier?](#6-ai-coordination-a-game-changer-or-a-risk-multiplier)
	- [6.1 Potential for AI coordination](#61-potential-for-ai-coordination)
	- [6.2 Challenges to develop coordination](#62-challenges-to-develop-coordination)
- [7. Rethinking AI safety](#7-rethinking-ai-safety)
	- [7.1 Balancing Speculative and Immediate Risks](#71-balancing-speculative-and-immediate-risks)
	- [7.2 Developing clearer definitions and frameworks](#72-developing-clearer-definitions-and-frameworks)
	- [7.3 Multidisciplinary approaches for AI safety](#73-multidisciplinary-approaches-for-ai-safety)
## 1. Introduction 
Artificial intelligence has rapidly evolved from a fascinating concept to a transformative force reshaping industries and daily life. From language models that generate human-like text to systems capable of artistic creativity or solving complex problems, the potential of AI seems infinite. Yet, alongside this excitement, a growing sentiment of fear has emerged, warnings about uncontrolled AI systems, existential risks, and the need for strict regulations. While these concerns deserve thoughtful attention, they often risk overshadowing the incredible progress and possibilities AI offers. This article explores the challenges in the current AI safety discourse and argues for a balanced perspective that avoids slowing down innovation while addressing genuine risks.

### 1.1 The promise and peril of AI
Artificial intelligence stands at the forefront of technological progress, promising a lot of benefits in almost every domains. Its applications are already visible: automating routine tasks, optimizing complex systems, and even unlocking new frontiers in areas like [healthcare](https://deepmind.google/discover/blog/codoc-developing-reliable-ai-tools-for-healthcare/), [education](https://blog.duolingo.com/large-language-model-duolingo-lessons/), and [climate modeling](https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/). These advancements could lead to unprecedented efficiency, enhanced creativity, and solutions to problems previously thought impossible to solve.

At the same time, the rapid pace of AI development raises legitimate concerns. Critics highlight potential dangers such as biased decision-making, misuse by malicious actors, and the disruption of labor markets. Some even foresee scenarios where AI surpasses human intelligence, leading to a loss of control. These fears, while important to consider, must not overshadow the tangible progress being made today. Instead of focusing on hypothetical existential risks, we should ensure that innovation thrives while addressing real and present challenges responsibly.

### 1.2 The current state of the debate
The conversation around AI safety has become increasingly polarized. On one side, some argue that AI could soon spiral out of human control, necessitating immediate and strict regulation to mitigate existential risks. This perspective often highlights scenarios involving "superintelligence" or autonomous systems acting against human interests. On the other side, there are those who view these fears as speculative, warning that premature regulation might reduce innovation and slow down progress in a field that could bring immense benefits to humanity.

This division is reflected even among the pioneers of deep learning. Geoffrey Hinton, often referred to as one of the "godfathers of deep learning," has publicly expressed concerns about AI's potential dangers, aligning himself with the so-called "AI doomers." In contrast, Yann LeCun, another foundational figure in the development of deep learning, strongly criticizes these alarmist views. LeCun argues that the risks are overstated and that such narratives could unnecessarily hinder the progress of AI research.

What complicates the debate further is the lack of consensus on the nature of the risks. Are we worried about near-term issues like biased algorithms, economic disruption, and cybersecurity vulnerabilities? Or are we prioritizing hypothetical, long-term scenarios where artificial intelligence could become more powerful than humanity itself? This divergence of focus has made it difficult to strike a balance between preparing for future challenges and addressing present-day problems.

This article argues that current AI safety efforts often misdirect attention toward speculative risks, diverting resources and energy from tackling practical challenges. By doing so, we risk hampering progress in a field that has the potential to transform our world for the better. It is crucial to examine these debates critically and advocate for an approach that fosters both innovation and responsibility.
In the following, we will delve into this question in detail, starting with the question of whether superintelligence is even possible, followed by an exploration of its timelines, potential dangers, and overlooked dimensions of AI safety. Finally, we will propose a balanced path forward that ensures progress while addressing legitimate concerns.

## 2. Can superintelligence even exist?
The possibility of machines surpassing human intelligence has long captured the imagination of researchers, policymakers, and the public. This idea, dramatized in movies like _The Terminator_ or inspired by Isaac Asimov's _Robot series_, conjures both amazement and fear, depicting a future where artificial entities far exceed human capabilities in every domain. For some, this vision represents boundless potential for innovation; for others, it raises deep concerns about the loss of control over such powerful systems. These cultural narratives have shaped public perception, amplifying fears of runaway intelligence and dystopian outcomes. Yet, before debating how to regulate or manage such systems, we must address a fundamental question: is this vision even realistic? Skeptics argue that superintelligence may be inherently unattainable or that its emergence is far more complex and uncertain than these dramatic works suggest. In the following, we will explore their arguments and what they mean for the broader debate on AI safety.

### 2.1 Skepticism About AI Superintelligence
The idea that machines could achieve superintelligence has its share of skeptics. Some argue that true intelligence may be intrinsically tied to biological processes, making it impossible for artificial systems to replicate. This perspective suggests that the human brain relies on unique mechanisms (possibly even quantum effects) that cannot be reproduced by digital computation. Others believe that human cognition involves qualities like creativity or common sense that are so deeply complex and poorly understood that modeling them within a machine is infeasible.

Moreover, there is skepticism about whether intelligence itself can generalize across different domains. Today’s AI systems excel at specific tasks, but their achievements are limited to narrow, well-defined areas. This lack of adaptability raises questions about whether artificial intelligence could ever match the broad, flexible intelligence of humans, let alone surpass it. From this viewpoint, the fear of uncontrollable, hyperintelligent AI feels less like a concrete reality and more like a misunderstanding of what current technologies are capable of achieving.

Shaped by such arguments, critics often see predictions of machine superintelligence as speculative, disconnected from the actual trajectory of AI research and its demonstrated limitations. However, while these doubts deserve serious consideration, they also risk underestimating the capacity of technology to evolve in ways that defy historical constraints.

### 2.2 Why superintelligence remains elusive
While skeptics raise important points about the challenges of achieving machine intelligence, dismissing its potential entirely overlooks the dynamic and often unpredictable nature of technological progress. History is full of breakthroughs that seemed unattainable until they were achieved. Neural networks, for instance, have already demonstrated [emergent capabilities](https://www.quantamagazine.org/the-unpredictable-abilities-emerging-from-large-ai-models-20230316/) beyond what their creators initially anticipated, hinting that AI systems can evolve in surprising ways. However, it is essential to temper this optimism with a clear recognition of the fundamental limitations we currently face.

One critical limitation is our incomplete understanding of the human brain. Intelligence, as we observe it in humans, arises from a highly complex interplay of continuous biological processes, many of which remain mysterious. Current AI architectures, built on discrete parameters and methods like backpropagation, are far away from the continuous, organic nature of neural activity in the brain. In my opinion, without a paradigm shift in how we approach AI development, such as moving away from discrete weight systems or exploring new organic or hybrid computing platforms, achieving true intelligence in machines seems far off.

Another significant barrier is the physical limitation of computing power. Scaling up today’s models relies heavily on brute force: larger datasets, more parameters, and greater computational resources. This approach is not only unsustainable but also fails to address the qualitative leap required to mimic the adaptability and efficiency of human cognition. True progress may demand breakthroughs in hardware, possibly through new forms of computation that better emulate biological systems.

In this context, the path to superintelligence becomes a question not of incremental improvements to existing methods but of bold innovation. While current advancements like multitask and multimodal systems are promising, they remain narrow in scope and fail to address the fundamental issues highlighted here. To bridge the gap between today’s AI and a truly intelligent system, we must look beyond traditional paradigms and embrace entirely new ways of thinking about intelligence, computation, and learning.


The challenges in achieving true intelligence in machines, from biological complexity to computational constraints, suggest that we are still far from realizing the kind of superintelligence often depicted in science fiction. However, even if we accept the possibility of achieving such breakthroughs in the future, a pressing question remains: how soon could this happen? Understanding the timelines for advanced AI development is crucial for framing the risks and opportunities ahead.

## 3. How Imminent Is the Risk?
Predicting the arrival of artificial general intelligence (AGI) has sparked interest, with researchers and enthusiasts offering forecasts based on current trends in AI development. On the platform Metaculus, over 1,400 forecasters have contributed to a detailed poll on [this question](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/). The probability density graph reveals a notable peak around 2028, with the mean and median predictions centering on 2032, just a few years from today. 
![[metaculus.png]]
Such forecasts reflect growing confidence in the rapid pace of AI progress, but they also raise critical questions: are these timelines realistic, or do they overlook the significant obstacles that remain? To address this, we will explore the arguments for and against these near-term predictions and assess the factors influencing the timeline for AGI.

### 3.1 Why predictions of AGI are premature
The confidence in near-term predictions of AGI, as seen in platforms like [Metaculus](https://www.metaculus.com), often relies on the current trajectory of AI advancements. Proponents point to rapid improvements in large language models, multimodal systems, and the exponential growth of computational resources as indicators that AGI could emerge within the next decade. However, this optimism frequently underestimates the fundamental challenges that still stand in the way.

One key issue is the physical limitation of scaling. Progress in AI has largely been driven by increasing computational power and model sizes, but this approach faces diminishing returns. Training larger models requires exponentially more resources, and there are finite limits to hardware capabilities and energy availability. Without breakthroughs in how we design and optimize AI systems, scaling alone will likely plateau before achieving general intelligence, if not [already](https://www.reuters.com/technology/artificial-intelligence/openai-rivals-seek-new-path-smarter-ai-current-methods-hit-limitations-2024-11-11/).

Another overlooked factor is the role of data. Current AI systems rely heavily on vast datasets curated from human-generated content, much of which has already been utilized. As we approach the limits of available high-quality data, creating systems capable of general reasoning will require new approaches, such as generating synthetic data or fundamentally rethinking how models learn.

Additionally, the timeline predictions often assume a linear progression of breakthroughs. In reality, progress in AI has been marked by periods of rapid development followed by plateaus. For instance, after the initial success of neural networks in the 1980s, the field experienced a "winter" of slowed progress before the resurgence of deep learning in the 2010s. Predicting a smooth path to AGI ignores the possibility of similar disruptions.

Finally, achieving general intelligence is not simply a matter of more powerful systems. It requires understanding and replicating the flexibility, creativity, and adaptability of human cognition, qualities that current AI lacks and that may demand a paradigm shift in how we build these systems.
While the forecasts on Metaculus are intriguing and reflect genuine excitement about the field, they may also reflect an overestimation of how close we are to solving these foundational challenges. In the context of these limitations, the confidence in AGI arriving by 2032 might be premature.

### 3.2 The problem with predictive timelines
Predictions on technology are historically too optimistic, and predicting when we will achieve AGI is no exception.

Historical predictions of flying cars or nuclear fusion, for instance, underestimated the complexity of achieving those milestones. Robin Hanson, in his article ["AI Risk, Again"](https://www.overcomingbias.com/p/ai-risk-again), argues that technological revolutions typically occur gradually and over time, not through sudden, uncontrollable leaps.
Hanson highlights that historical transformations, like the Industrial Revolution, unfolded over decades. Even when groundbreaking innovations occurred, they led to widespread changes through incremental adoption and refinement, not abrupt use. He suggests that if AGI emerges, it will likely follow a similar path: steady progress rather than a rapid "foom" scenario, where one AI system surpasses all others overnight.

In my opinion, progress in AI has already begun to slow down, though this may not be immediately obvious due to the current hype around transformers and large language models. While transformers have proven highly effective in tasks like language, image, and video processing, I believe **transformer is not all you need.** The dominance of transformers has created a narrow focus in the research community, leading to an overreliance on scaling existing architectures instead of exploring fundamentally new ideas. This obsession with transformers risks delaying the next significant breakthrough in AI. To truly move toward AGI, we need to step beyond transformers and attention-based mechanisms. However, I believe this shift will not happen until the hype around transformers subsides, which I think could take at least 15 years.

Hanson’s argument reinforces this view: transformative breakthroughs rarely come from refining existing methods alone. They require time, exploration, and the willingness to abandon popular approaches when they reach their limits. Until the research community moves past the transformer era, progress in AI will likely remain incremental, making predictions of near-term AGI overly optimistic. 

While the timelines for achieving general intelligence remain uncertain, the debate is not just about when AGI might emerge but also how dangerous it could be. Even if AGI is far away (or unlikely to ever occur), discussions about the risks of AI systems are already happening. Understanding whether these risks are overblown or really alarming is critical for guiding how we approach AI safety and development.

## 4. How dangerous is AI, really?
Discussions about AI risks often focus on worst-case scenarios, but how real are these dangers, and how should we address them?
### 4.1 Misuse risks
#### 4.1.1 Examples of misuse
While much of the AI safety debate centers on speculative risks, there are immediate and tangible ways in which AI systems could be misused. Cyberattacks, for instance, are an ever-present concern. AI tools capable of generating realistic phishing emails, cracking passwords, or automating network infiltration can significantly amplify the scale and sophistication of cybercrime. These threats are not theoretical; they are actively evolving alongside AI technology.

Another area of concern is the spread of misinformation. AI-generated content, such as deepfakes or made up text, has the potential to harm at unprecedented levels. Political campaigns, social movements, or even individual reputations can be manipulated by targeted disinformation campaigns, eroding public trust in information sources and institutions.

The use of autonomous weapons represents another alarming possibility. AI-controlled drones or other military systems could lower the barrier to initiating conflicts by reducing the need for human involvement in combat. Furthermore, the deployment of such systems by non-state entities could destabilize global security in many ways.

Bioterrorism is also frequently discussed in the context of AI misuse. Tools for analyzing genetic data or automating biological experiments could, in theory, be used to engineer harmful pathogens. While this risk remains speculative for now, the dual-use nature of many AI applications in biology warrants careful monitoring and regulation.

These examples highlight that AI’s misuse potential is not confined to far-future scenarios. Addressing these challenges requires vigilance, transparency, and well-thought-out policies to mitigate harm without stifling progress.

#### 4.1.2 Are some risks exaggerated?
While the misuse of AI poses real and immediate dangers, some scenarios appear significantly unlikely. A good example is the fear that AI systems could design and deploy bioweapons on its own, with minimal human intervention. These concerns, while worth considering, frequently lack practical evidence to support their feasibility.

For instance, the idea that AI could autonomously create bioweapons assumes a level of understanding and intent that current systems simply do not possess. AI tools used in biology or chemistry are highly task-specific and require human supervision for most processes. Even with access to vast datasets, the complexity of designing and testing harmful pathogens involves steps that go far beyond the capabilities of existing AI systems.

Similarly, large-scale manipulation through AI-generated misinformation often assumes an audience that is unable to critically evaluate content. While deepfakes and fabricated text can are indeed a danger, their effectiveness tends to decrease with increased public awareness and the development of detection tools. Efforts to improve media literacy and implement technical safeguards further reduce the potential impact of such campaigns.

These exaggerated fears might drive attention and resources away from addressing the more immediate and tangible risks discussed earlier. By focusing too heavily on worst-case scenarios, we may overlook practical solutions to current challenges that are already affecting societies and industries.

### 4.2 Conceptual issues: defining safety and alignment
One of the obstacles in AI safety discussions is the lack of precise and universally accepted definitions for fundamental concepts like "human-level AI", "superintelligence", and "alignment". These terms, while commonly used, carry varying interpretations depending on the context, leading to confusion and misaligned goals among researchers, policymakers, or industry leaders.

For example, "human-level AI" often means an AI system that can match human capabilities across all domains. However, this definition raises more questions than it answers. Does it imply replicating human cognitive processes or just achieving equivalent outcomes? Should it include emotional intelligence, creativity, and ethical reasoning, or only technical proficiency? Without clarity, efforts to prepare for or regulate such systems risk being inconsistent.

Similarly, "alignment" is a term that contains a large and complex range of ideas. At its core, alignment refers to ensuring that AI systems act in ways consistent with human values and goals. Yet, human values are not rigid: they vary across cultures, individuals, and contexts. Aligning AI to a single set of values risks oversimplification, while attempting to account for all possible value systems introduces great complexity. Furthermore, even if alignment is achieved during development, maintaining it as systems evolve and interact with the world presents another challenge.

These conceptual ambiguities extend to the idea of "safety." Some define safety as ensuring AI systems do not harm humans or the environment, while others see it as maintaining control over AI behavior. The lack of consensus on these definitions means that research efforts are not unified, with different teams addressing different interpretations of what safety entails.

In my opinion, this vagueness is one of the most problematic issues in the AI safety discourse. Without clear definitions, it becomes difficult to design robust frameworks for risk mitigation or to communicate effectively between technical researchers and policymakers. A first step toward addressing this issue could involve creating global definitions for these key terms, informed by interdisciplinary collaboration between technical experts, ethicists, and social scientists. Only then can the field move forward with a shared understanding of what we want to achieve.

AI risks span immediate misuse and long-term concerns, but both are clouded by unclear definitions of key concepts like safety and alignment. Bridging these gaps is essential for moving beyond speculative fears and addressing real challenges effectively, paving the way for a clearer and more practical AI safety discourse.

### 4.3 Why intelligence alone may not be that dangerous
While much of the AI safety debate focuses on the dangers of intelligent systems, it’s important to recognize that intelligence does not inherently mean power or threat. The most intelligent humans are not necessarily the most influential or dangerous. Similarly, AI systems, no matter how advanced, require access to physical systems or resources to initiate changes in the world.

Current AI lacks autonomy and remains dependent on human inputs for deployment and action. Even highly capable models operate within predefined constraints, with their outputs limited by the environments in which they are used. This dependence questions the assumption that intelligence alone could lead to catastrophic scenarios without human oversight or intervention.

While much of the AI safety discourse centers on speculative risks and technical challenges, it often fais to see broader societal and environmental dimensions. To create a truly comprehensive approach to AI safety, we must address these neglected areas.

## 5. Neglected dimensions of AI safety
AI’s influence extends far beyond technical capabilities, shaping economies, societies, and even the environment. These dimensions of AI impact are often overshadowed by debates about existential risks, but they are no less important. Addressing these neglected areas is critical to ensuring that AI development aligns with larger human and planetary well-being, encouraging progress that benefits everyone sustainably.
### 5.1 Societal and regulatory oversights
The rapid advancement of artificial intelligence has introduced significant societal challenges that require careful consideration. One immediate concern is the economic impact of AI on labor markets. As automation continues to replace jobs across various sectors, issues of job displacement and economic inequality become increasingly pressing. Without proactive measures, the benefits of AI risk being concentrated among a select few, exacerbating existing disparities.

Regulatory challenges further complicate the landscape. Policies driven by fear of speculative AI risks can inadvertently reduce innovation. Overly restrictive regulations may deter smaller organizations or researchers from pursuing AI projects, leading to a concentration of power within a few large tech companies. This centralization not only hurts progress but also raises ethical and practical concerns regarding the control and deployment of AI technologies.

Moreover, the global race for AI dominance is reshaping international power dynamics. Nations striving to lead in AI development may inadvertently foster geopolitical tensions and create disparities in access to AI's benefits. Countries lagging in AI capabilities could face growing economic and political disadvantages, deepening global inequality.

Addressing these societal and regulatory challenges necessitates a balanced approach that safeguards against real risks without hindering innovation. Forward-thinking policies should promote equitable AI development and encourage global cooperation, ensuring that AI's transformative potential benefits all of humanity. As highlighted by [Lakera](https://www.lakera.ai/blog/risks-of-ai), understanding and mitigating the multifaceted risks of AI is crucial in our increasingly AI-driven world.

### 5.2 Environmental costs
The [environmental costs of AI development](https://oecd.ai/en/wonk/understand-environmental-costs) represent another critical dimension of safety, one that impacts the long-term sustainability of the planet. The rapid scaling of AI, particularly through large-scale models, requires immense data storage and processing power, leading to significant energy consumption and carbon emissions. For example, "training a general-purpose AI model now requires about 10 billion times more computational power than state-of-the-art models in 2010", with the energy demands projected to rival the annual electricity consumption of countries like Austria or Finland by 2026.

AI’s environmental costs are both direct and indirect. Direct impacts include the lifecycle emissions of AI compute resources (production, operation, and disposal stages) which heavily affect energy consumption and water resources. Indirect impacts, such as those resulting from AI applications in smart grids or precision agriculture, can be positive but also introduce risks like unsustainable consumption patterns. Quantifying these effects remains a challenge, partly due to the lack of standardized methods for measuring AI-specific energy use.

Data management is another critical factor. Training AI systems relies on vast datasets, yet inefficient storage practices and the proliferation of synthetic data contribute to unnecessary energy use and emissions. Studies indicate that up to 80% of business data stored on-premise is redundant or obsolete. Adopting data minimization practices, prioritizing efficient storage, and responsibly disposing of unused data could significantly reduce AI’s environmental footprint.

To mitigate these issues, the AI community must embrace environmentally responsible design choices. This includes evaluating the necessity of large general-purpose models for specific tasks in favor of simpler, less resource-intensive alternatives where feasible. Open-source datasets, decentralized training methods, and reducing reliance on new data creation also present opportunities for improving sustainability. Policymakers, researchers, and industry leaders must collaborate to embed these principles into AI lifecycle management, aligning the technology’s growth with broader goals of digital decarbonization.

By recognizing and addressing its environmental costs, AI can progress responsibly, ensuring that its benefits do not come at the expense of the planet’s future. This expanded view of AI safety incorporates not only societal and technical considerations but also the sustainability of the ecosystems upon which humanity depends.

## 6. AI coordination: a game-changer or a risk multiplier?
### 6.1 Potential for AI coordination
A central debate in AI safety is whether AI systems will have a unique ability to [coordinate](https://www.lesswrong.com/posts/gYaKZeBbSL4y2RLP3/strategic-implications-of-ais-ability-to-coordinate-at-low) at low cost and large scale, as some theorists suggest, or if they will remain fundamentally uncoordinated entities. If AIs lack this "special ability," the development of many independent systems could naturally lead to a **multipolar scenario**: a landscape where no single AI dominates and power remains distributed among multiple agents.

In a multipolar scenario, the risks associated with unaligned or competing systems may actually be mitigated. Without the ability to merge utility functions or form centralized networks, AIs would need to operate within their distinct parameters, relying on external mechanisms, like laws and governance systems, to maintain order. This reduces fears of a "singleton" AI or runaway superintelligence consolidating power and bypassing human oversight.

Supporters of AI coordination argue that systems capable of merging utility functions could form efficient, self-regulating networks, reducing conflict and redundancy. However, this assumes breakthroughs in coordination technologies that may not materialize. As Wei Dai highlights, the technical challenges of aligning goals across independent systems make seamless AI coordination unlikely. This suggests that AIs, like human entities, will more likely compete than cooperate in ways that fundamentally alter the power balance.

From a governance perspective, the absence of unique AI coordination capabilities has significant implications. If AIs remain decentralized and competitive, existing regulatory frameworks, focused on transparency, interoperability, and competition, may be sufficient to manage risks. Heavy-handed regulation aimed at addressing fears of unified AI coordination could slow down innovation without addressing actual threats. Instead, policies should encourage the development of systems that are robust, diverse, and compatible with human oversight.

The multipolar scenario offers a pragmatic lens through which to view the future of AI development. It challenges the assumption that AIs will inherently consolidate power and underscores the importance of fostering a competitive yet cooperative environment where innovation thrives without jeopardizing societal stability.

### 6.2 Challenges to develop coordination
While the idea of AI coordination may seem plausible in theory, ensuring such coordination faces significant practical and technical obstacles. These challenges highlight why a multipolar scenario is far more likely than a world dominated by unified, cooperative AI networks.

A major technical barrier to coordination is the difficulty of aligning goals across systems. Different AIs are designed with distinct architectures, purposes, and constraints, making the merging of utility functions or objectives inherently complex. Furthermore, emergent behaviors in AI systems can create conflicts rather than alignment.

[Corrigibility](https://www.alignmentforum.org/tag/corrigibility) adds another layer of complexity. Ensuring that AIs remain corrigible (allowing human oversight or modification) can limit their ability to form cooperative networks. A non-corrigible AI might refuse to integrate with other systems, creating a competitive dynamic that undermines large-scale coordination efforts. This tension between safety and competitiveness further reduces the feasibility of unified AI coordination.

The broader landscape of AI development also complicates coordination. Companies and nations are racing to establish dominance in AI technology, driven by economic and strategic incentives. This competitive environment fosters fragmentation rather than collaboration, with independent AIs optimized for individual goals rather than collective outcomes.

Given these challenges, assuming that AIs will naturally or easily coordinate is unrealistic. Instead, governance strategies should reflect the likely reality of a multipolar AI ecosystem, emphasizing transparency and interoperability rather than attempting to enforce centralized cooperation. Recognizing these limitations helps refocus AI safety efforts on practical and achievable objectives.

## 7. Rethinking AI safety
Rethinking AI safety requires moving beyond polarized debates and speculative fears to embrace a balanced, practical, and forward-thinking approach that addresses current challenges while preparing for future possibilities.
### 7.1 Balancing Speculative and Immediate Risks
AI safety discussions often rely heavily toward speculative scenarios involving superintelligence or catastrophic takeovers. While these concerns are intellectually stimulating, they risk diverting attention and resources from immediate, observable challenges. Issues like algorithmic bias, cybersecurity vulnerabilities, and systemic impacts on labor markets are not hypothetical: they are already affecting society.

Focusing on these tangible risks offers two key benefits. First, addressing practical issues builds trust in AI technologies, paving the way for responsible deployment and innovation. Second, solving immediate challenges can indirectly mitigate long-term risks. For example, improving robustness and transparency in AI systems today reduces the likelihood of unpredictable behaviors in more advanced systems.

Balancing speculative fears with practical concerns does not mean ignoring the future. Instead, it ensures that efforts to manage risks are grounded in reality and provide value now. By prioritizing actionable solutions, we can create a safer and more sustainable foundation for AI’s continued growth.

### 7.2 Developing clearer definitions and frameworks
One of the key obstacles in AI safety is the lack of precise definitions for critical concepts like "alignment," "safety," and even "intelligence." These terms, though widely used, often carry ambiguous or conflicting meanings across different fields. This lack of clarity hampers efforts to address risks well and leads to fragmented priorities.

What exactly does "alignment" mean? Does it involve strict adherence to universal human values, and if so, whose values are we prioritizing? Can we achieve a shared understanding of "safety" when interpretations range from technical robustness to ethical considerations? And what does it truly mean for an AI to be "intelligent"? 
Without answering these foundational questions, how can we effectively design frameworks to address risks and guide development?

To overcome these challenges, we need robust frameworks that establish shared meanings and evaluation criteria. These frameworks should not only define key terms but also offer standardized methods to test and measure the safety and alignment of AI systems. Interdisciplinary collaboration will be essential, bringing together technical researchers, ethicists, and policymakers to ensure these definitions are both comprehensive and actionable.

### 7.3 Multidisciplinary approaches for AI safety
AI safety requires more than technical solutions, it demands collaboration across disciplines. Researchers, policymakers, and industry leaders must work together to address risks while promoting innovation. Societal perspectives, such as the impact on labor markets and global inequalities, should also inform these efforts to ensure AI aligns with humanity’s broader goals.

Sustainability is another critical dimension. The environmental costs of AI, from energy usage to resource consumption, must be minimized through efficient algorithms and responsible deployment. By integrating diverse perspectives and prioritizing sustainability, we can create a balanced and comprehensive approach to AI safety.

---

To summarize and conclude, in my point of view, the discourse on AI safety must move beyond fear and speculation, focusing instead on practical solutions, clearer frameworks, and collaboration across disciplines. I believe that by addressing today’s challenges while preparing for tomorrow’s opportunities we can ensure that AI continues to serve humanity’s best interests.
